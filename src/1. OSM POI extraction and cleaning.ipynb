{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# OSM POI extraction and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['USE_PYGEOS'] = '0'\n",
    "import matplotlib.pyplot as plt\n",
    "# import leafmap.maplibregl as leafmap\n",
    "from shapely import Point, LineString, Polygon\n",
    "import geopandas as gpd\n",
    "import h3\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "from rasterstats import zonal_stats, point_query\n",
    "import rasterio\n",
    "import osmnx as ox\n",
    "import duckdb\n",
    "from rasterio.plot import show\n",
    "import numpy as np\n",
    "from rasterio import features\n",
    "import time\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Administrative boundaries\n",
    "cantons = gpd.read_file(\n",
    "    '../../SeroCOV/data/geo_units/swissBOUNDARIES3D_1_3_TLM_KANTONSGEBIET.shp', engine='pyogrio')\n",
    "communes = gpd.read_file('../../SeroCOV/data/geo_units/swissBOUNDARIES3D_1_3_TLM_HOHEITSGEBIET.shp', engine='pyogrio')\n",
    "# Only retain communes that are in the canton of Geneva\n",
    "communes = communes[communes.KANTONSNUM == 25]\n",
    "\n",
    "cantons = cantons.to_crs(2056)\n",
    "communes = communes.to_crs(2056)\n",
    "canton_ge = cantons[cantons.NAME=='Gen√®ve']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Define buffered area within which we will extract amenities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import box\n",
    "\n",
    "# Get the geometry of the place\n",
    "gdf = ox.geocode_to_gdf(place)\n",
    "\n",
    "# Create a buffer around the geometry\n",
    "buffered_gdf = gdf.to_crs(epsg=2056).buffer(4000).to_crs(gdf.crs)\n",
    "buffered_gdf.plot()\n",
    "# plt.savefig('./results/canton_ge_buffered.png', dpi=80)\n",
    "buffered_gdf = buffered_gdf.to_crs(4326)\n",
    "# Get the bounding box of the buffered area\n",
    "bounds = buffered_gdf.total_bounds\n",
    "bbox = box(*bounds)\n",
    "\n",
    "# Extract the network using the buffered area\n",
    "G_buffered = ox.graph_from_polygon(bbox, network_type='walk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_buffered= gpd.GeoDataFrame(buffered_gdf, crs = 4326, geometry = buffered_gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tags = {\"amenity\": True, \"landuse\": [\"retail\", \"commercial\"], \"highway\": \"bus_stop\"}\n",
    "# gdf_retail_bus = ox.features_from_place(place, tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Based on [Bruno et al.](https://static-content.springer.com/esm/art%3A10.1038%2Fs44284-024-00119-4/MediaObjects/44284_2024_119_MOESM1_ESM.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Healthcare POIs\n",
    "healthcare_pois_dict = {\n",
    "    'amenity': ['clinic', 'dentist', 'doctors', 'hospital', 'pharmacy', 'physiokinesitherapy'],\n",
    "    'healthcare': True,  # All healthcare tags\n",
    "    'healthcare:speciality': True,  # All healthcare specialities\n",
    "    'government': ['healthcare'],\n",
    "    'office': ['medical', 'physician'],\n",
    "    'social_facility': ['ambulatory_care', 'assisted_living', 'day_care', 'daycare', 'healthcare']\n",
    "}\n",
    "\n",
    "# Services POIs\n",
    "services_pois_dict = {\n",
    "    'amenity': [\n",
    "        'animal_boarding', 'animal_breeding', 'animal_shelter', 'atm', 'bank', 'bicycle_parking', \n",
    "        'bicycle_rental', 'bicycle_repair_station', 'boat_rental', 'boat_storage', 'bureau_de_change', \n",
    "        'car_rental', 'car_sharing', 'car_wash', 'casino', 'childcare', 'club', 'community_centre', \n",
    "        'community_centres', 'driving_school', 'internet_cafe', 'kindergarten', 'left_luggage', \n",
    "        'luggage_storage', 'marketplace', 'ministry', 'money_transfer', 'mortuary', 'newsagent', \n",
    "        'nightclub', 'nursing_home', 'parcel_locker', 'payment_terminal', 'photo_booth', 'photobooth', \n",
    "        'post_office', 'recording_studio', 'rescue_station', 'social_centre', 'social_facility', 'toilets'\n",
    "    ],\n",
    "    'bicycle_parking': ['building', 'lockers', 'rack', 'stands', 'wall_loops', 'wave'],\n",
    "    'building': ['kindergarten', 'nursing_home', 'post_office', 'public_administration', 'public_bath', 'service'],\n",
    "    'building:part': ['transportation'],\n",
    "    'craft': [\n",
    "        'camera_repair', 'car_repair', 'carpenter', 'caterer', 'cleaning', 'coachbuilder', 'confectionery',\n",
    "        'electrician', 'electronics_repair', 'gardener', 'handicraft', 'heating_engineer', 'hvac', 'joiner',\n",
    "        'key_cutter', 'laboratory', 'locksmith', 'optician', 'photographer', 'photographic_laboratory',\n",
    "        'plumber', 'printer', 'sculptor', 'shoemaker', 'stonemason', 'tailor', 'tiler', 'turner', 'upholsterer',\n",
    "        'watchmaker', 'window_construction'\n",
    "    ],\n",
    "    'leisure': ['arcade_hall', 'amusement_arcade', 'adult_gaming_centre', 'escape_game', 'recreation_center', 'sauna', 'tanning_salon'],\n",
    "    'office': [\n",
    "        'architect', 'architect;engineer', 'bank', 'employment_agency', 'employment_consultant', 'energy_supplier',\n",
    "        'engineer', 'engineering', 'estate_agent', 'financial', 'financial_advisor', 'foundation', 'government',\n",
    "        'insurance', 'labour_union', 'lawyer', 'legal', 'moving_company', 'ngo', 'oil;gas', 'public_administration',\n",
    "        'service', 'tax_advisor', 'taxi', 'telecommunication', 'travel_agent', 'wedding_planner'\n",
    "    ],\n",
    "    'service': [\n",
    "        'dealer; repair', 'electrical', 'parts', 'parts;repair', 'repair', 'tyres;repair'\n",
    "    ],\n",
    "    'service:bicycle:pump': ['yes'],\n",
    "    'service:bicycle:repair': ['yes'],\n",
    "    'service:vehicle:inspection': ['yes'],\n",
    "    'service:vehicle:car_repair': ['yes'],\n",
    "    'service:vehicle:tyres': ['yes'],\n",
    "    'shop': [\n",
    "        'dog_spa', 'dry_cleaning', 'electrical_repair', 'electronics_repair', 'estate_agent', 'finance',\n",
    "        'funeral_directors', 'hairdresser', 'internet', 'laundry', 'massage', 'model', 'money_lender',\n",
    "        'nails', 'newsagent', 'newspaper_agent', 'pet_grooming', 'photo', 'printing', 'rental', 'scrap_yard',\n",
    "        'shoe_repair', 'tattoo', 'travel_agency'\n",
    "    ],\n",
    "    'social_facility': ['food_bank', 'group_home', 'leisure', 'nursing_home', 'senior_citizen_centre'],\n",
    "    'street_cabinet': ['postal_service'],\n",
    "    'tourism': ['spa_resort', 'theme_park']\n",
    "}\n",
    "\n",
    "# Transport POIs\n",
    "transport_pois_dict = {\n",
    "    'amenity': ['bus_station', 'ferry_terminal'],\n",
    "    'building': ['subway_station', 'transportation'],\n",
    "    'building:part': ['transportation'],\n",
    "    'bus': ['urban', 'yes'],\n",
    "    'car_sharing': ['yes'],\n",
    "    'construction': ['subway_station'],\n",
    "    'departures_board': ['no', 'realtime', 'timetable', 'yes'],\n",
    "    'government': ['transportation'],\n",
    "    'public_transport': ['platform', 'station', 'stop', 'stop_area', 'stop_position'],\n",
    "    'railway': ['platform', 'station', 'stop', 'stop;station', 'subway_entrance', 'tram_stop'],\n",
    "    'shelter_type': ['public_transport'],\n",
    "    'station': ['light_rail', 'rail', 'subway', 'train'],\n",
    "    'subway': ['yes'],\n",
    "    'suspended:highway': ['bus_stop'],\n",
    "    'taxi': ['designated', 'yes'],\n",
    "    'train': ['yes'],\n",
    "    'tram': ['yes'],\n",
    "    'trolleybus': ['yes'],\n",
    "    'waterway': ['boatyard', 'dock']\n",
    "}\n",
    "\n",
    "# Outdoor POIs\n",
    "outdoor_pois_dict = {\n",
    "    'amenity': ['bench', 'watering_place'],\n",
    "    'attraction': [\n",
    "        'amusement_ride', 'animal', 'big_wheel', 'bumper_boats', 'bumper_car', 'bumper_cars',\n",
    "        'carousel', 'carriage_ride', 'dark_ride', 'drop_tower', 'haunted_house', 'maze',\n",
    "        'playset', 'pneumatic_slide', 'roller_coaster', 'slide', 'swing_carousel', 'trampoline',\n",
    "        'water_ride', 'water_slide'\n",
    "    ],\n",
    "    'bench': ['yes'],\n",
    "    'denotation': ['park'],\n",
    "    'landuse': ['recreation_ground'],\n",
    "    'leisure': [\n",
    "        'beach_resort', 'bleachers', 'dog_park', 'fishing', 'garden', 'ice_rink', 'marina',\n",
    "        'miniature_golf', 'nature_reserve', 'park', 'picnic_table', 'pitch', 'playground',\n",
    "        'recreation_ground', 'water_park'\n",
    "    ],\n",
    "    'natural': ['beach', 'cave', 'cave_entrance', 'cliff', 'park'],\n",
    "    'playground': [\n",
    "        'roundabout', 'seesaw', 'slide', 'slide;spinny;swing', 'springy', 'structure'\n",
    "    ],\n",
    "    'playground:theme': ['playground'],\n",
    "    'shelter_type': ['picnic_shelter'],\n",
    "    'site_type': ['stadium'],\n",
    "    'tourism': ['picnic_site', 'viewpoint', 'zoo'],\n",
    "    'water': ['Piscina', 'fishpond', 'lake', 'pond']\n",
    "}\n",
    "\n",
    "# Supplies POIs\n",
    "supplies_pois_dict = {\n",
    "    'amenity': ['compressed_air', 'fuel', 'gambling;vending_machine', 'marketplace', 'vending_machine'],\n",
    "    'building': ['mall', 'market', 'retail', 'retail_outlet', 'shopping_mall', 'supermarket'],\n",
    "    'building:part': ['retail', 'supermarket'],\n",
    "    'building:use': ['commercial'],\n",
    "    'craft': ['distillery', 'dressmaker', 'glaziery', 'goldsmith', 'parquet_layer', 'pottery', 'winery'],\n",
    "    'fuel:cng': ['yes'],\n",
    "    'fuel:diesel': ['yes'],\n",
    "    'fuel:octane_95': ['yes'],\n",
    "    'fuel:octane_98': ['yes'],\n",
    "    'goods': ['yes'],\n",
    "    'landuse': ['retail'],\n",
    "    'second_hand': ['no', 'only', 'yes'],\n",
    "    'service': ['parts', 'parts;repair'],\n",
    "    'service:bicycle:retail': ['yes'],\n",
    "    'shop': [\n",
    "        'accessories', 'alcohol', 'anime', 'antiques', 'appliance', 'art', 'artist', 'baby_goods',\n",
    "        'bag', 'bakery', 'bathroom_furnishing', 'beauty', 'beauty; piercing', 'bed', 'beverages',\n",
    "        'bicycle', 'bookmaker', 'books', 'boutique', 'building_supplies', 'butcher', 'camera',\n",
    "        'cannabis', 'car', 'car_hifi', 'car_parts', 'caravan', 'carpet', 'charity', 'cheese',\n",
    "        'chemist', 'chocolate', 'clothes', 'clothes;tailor', 'coffee', 'collector', 'comics',\n",
    "        'communications', 'computer', 'confectionery', 'construction_supplies', 'convenience',\n",
    "        'copyshop', 'cosmetics', 'costumes', 'craft', 'curtain', 'dairy', 'deli', 'dental_supplies',\n",
    "        'department_store', 'doityourself', 'doors', 'duty_free', 'e-cigarette', 'electric_supplies',\n",
    "        'electrical', 'electronics', 'engraver', 'erotic', 'fabric', 'fair_trade', 'farm', 'fashion',\n",
    "        'fashion_accessories', 'fireworks', 'fishing', 'fishmonger', 'flooring', 'florist', 'food',\n",
    "        'frame', 'frozen_food', 'fuel', 'furniture', 'games', 'general', 'gift', 'glazery', 'glaziery',\n",
    "        'greengrocer', 'grocery', 'haberdashery', 'hairdresser_supply', 'hardware', 'hat', 'health_food',\n",
    "        'hearing_aids', 'heating', 'heating_cooling', 'herbalist', 'hifi', 'hobby', 'household_goods',\n",
    "        'household_linen', 'houseware', 'hunting', 'interior_decoration', 'jewelry', 'kiosk', 'kitchen',\n",
    "        'leather', 'lighting', 'locksmith', 'lottery', 'mall', 'medical_supply', 'metal construction craft',\n",
    "        'mobile_phone', 'motorcycle', 'motorcycle_repair', 'music', 'musical_instrument', 'negozio_di_lampade',\n",
    "        'noodles', 'nutrition_supplements', 'optician', 'orthopedics', 'outdoor', 'paint', 'party', 'pasta',\n",
    "        'pastry', 'pawnbroker', 'perfumery', 'pet', 'pottery', 'printer_ink', 'pyrotechnics', 'radiotechnics',\n",
    "        'religion', 'scuba_diving', 'seafood', 'second_hand', 'security', 'sewing', 'shoes', 'sports',\n",
    "        'stationery', 'supermarket', 'tailor', 'tea', 'telecommunication', 'ticket', 'tiles', 'tobacco',\n",
    "        'toys', 'trade', 'truck', 'tyres', 'vacuum_cleaner', 'variety_store', 'video', 'video_games',\n",
    "        'watches', 'weapons', 'wholesale', 'window_blind', 'wine', 'wood', 'workshop', 'yes'\n",
    "    ],\n",
    "    'use': ['market'],\n",
    "    'vending': True  # All vending tags\n",
    "}\n",
    "\n",
    "fast_food_pois_dict = {'amenity':'fast_food',\n",
    "                      'building':'fast_food_restaurant'}\n",
    "# Restaurant POIs\n",
    "restaurant_pois_dict = {\n",
    "    'amenity': [\n",
    "        'bbq', 'bar', 'biergarten', 'cafe', 'canteen', 'fast_food', 'food_court', 'ice_cream',\n",
    "        'kitchen', 'pub', 'restaurant'\n",
    "    ],\n",
    "    'bar': True,  # All bar tags\n",
    "    'building': ['fast_food_restaurant'],\n",
    "    'cuisine': True,  # All cuisine tags\n",
    "    'cuisine_1': True,  # All cuisine_1 tags\n",
    "    'cuisine_2': True,  # All cuisine_2 tags\n",
    "    'diet': True,  # All diet tags\n",
    "    'diet:vegan': True,  # All diet:vegan tags\n",
    "    'diet:vegetarian': True,  # All diet:vegetarian tags\n",
    "    'food': ['ice_cream'],\n",
    "    'ice_cream': ['artisanal', 'industrial', 'yes'],\n",
    "    'ice_cream:type': True,  # All ice_cream:type tags\n",
    "    'oven': ['electrical', 'wood_fired'],\n",
    "    'shop': ['cafe', 'ice_cream', 'pizza'],\n",
    "    'takeaway': ['no', 'only', 'yes'],\n",
    "    'tourism': ['agritourism']\n",
    "}\n",
    "\n",
    "# Culture POIs\n",
    "culture_pois_dict = {\n",
    "    'amenity': [\n",
    "        'art_gallery', 'arts_centre', 'baptistery', 'cinema', 'conference_centre', 'convention_centre',\n",
    "        'events_venue', 'library', 'monastery', 'music_school', 'planetarium', 'public_bookcase',\n",
    "        'theatre', 'ticket_office', 'toy_library'\n",
    "    ],\n",
    "    'attraction': ['historic', 'train'],\n",
    "    'artwork_type': True,  # All artwork_type tags\n",
    "    'building': [\n",
    "        'abbey', 'arena', 'baptistery', 'basilica', 'castle', 'cinema', 'congress_centre', 'gasometer',\n",
    "        'library', 'mausoleum', 'monastery', 'museum', 'obelisk', 'place_of_worship', 'propylaea',\n",
    "        'pyramid', 'quadrifrons', 'sports_arena', 'stadium', 'temple', 'theatre'\n",
    "    ],\n",
    "    'building:part': ['castle', 'theatre'],\n",
    "    'club': ['culture'],\n",
    "    'fountain': ['sarcophagus', 'scenic'],\n",
    "    'government': ['culture'],\n",
    "    'historic': True,  # All historic tags\n",
    "    'historic:civilization': True,  # All historic:civilization tags\n",
    "    'historic:period': True,  # All historic:period tags\n",
    "    'landuse': ['observatory'],\n",
    "    'leisure': ['stadium'],\n",
    "    'memorial': [\n",
    "        'bust', 'column', 'fountain', 'ghost_bike', 'obelisk', 'plaque', 'sculpture', 'statue',\n",
    "        'stele', 'stolperstein', 'stopelstein', 'war', 'war_memorial', 'yes'\n",
    "    ],\n",
    "    'museum': True,  # All museum tags\n",
    "    'period': ['aurelian'],\n",
    "    'ruins': [\n",
    "        'acqueduct', 'baths', 'castle', 'cemetery', 'crepidoma', 'mausoleum', 'temple', 'thermae', 'tomb'\n",
    "    ],\n",
    "    'site_type': [\n",
    "        'castle', 'catacomb', 'catacombs', 'domus', 'fortification', 'necropolis', 'nymphaeum',\n",
    "        'roman_circus', 'roman_road', 'roman_villa', 'ruins', 'temple'\n",
    "    ],\n",
    "    'tomb': ['mausoleum', 'memorial', 'sarcophagus', 'war_grave'],\n",
    "    'tourism': ['aquarium', 'artwork', 'attraction', 'gallery', 'museum']\n",
    "}\n",
    "\n",
    "# Education POIs\n",
    "education_pois_dict = {\n",
    "    'amenity': ['college', 'fablab', 'language_school', 'school', 'university'],\n",
    "    'building': ['college', 'foreign_school', 'school', 'school;yes', 'university'],\n",
    "    'building:part': ['school'],\n",
    "    'distretto_scolastico': True,  # All distretto_scolastico tags\n",
    "    'government': ['education'],\n",
    "    'landuse': ['education'],\n",
    "    'office': ['educational_institution', 'research', 'research_institute', 'school']\n",
    "}\n",
    "\n",
    "# Physical POIs\n",
    "physical_pois_dict = {\n",
    "    'amenity': ['Lasergame', 'dancing_school', 'dive_centre', 'dojo'],\n",
    "    'boules': ['bocce', 'boccia', 'yes'],\n",
    "    'building': ['gym', 'gymnasium', 'sports_centre', 'sports_hall'],\n",
    "    'club': ['sport'],\n",
    "    'dance:teaching': ['yes'],\n",
    "    'leisure': [\n",
    "        'bowling_alley', 'dance', 'fitness_centre', 'fitness_station', 'golf_course', 'horse_riding',\n",
    "        'racetrack', 'sports_centre', 'sports_hall', 'swimming_pool', 'track', 'yoga'\n",
    "    ],\n",
    "    'natural': ['peak'],\n",
    "    'playground': ['climbingframe', 'climbingwall', 'zipwire'],\n",
    "    'shop': ['yoga'],\n",
    "    'sport': True,  # All sport tags\n",
    "    'swimming_pool': ['covered', 'indoor', 'outdoor', 'yes']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_pois_from_tag_dict(place_name, tag_dict, timeout=15, pause=2):\n",
    "    \"\"\"\n",
    "    Fetch points of interest by iterating through tag categories and concatenating results.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    place_name : str\n",
    "        The name of the place to query (e.g., 'Grand Gen√®ve')\n",
    "    tag_dict : dict\n",
    "        Dictionary of tag categories and their values\n",
    "    timeout : int, optional\n",
    "        Timeout for API requests in seconds, defaults to 180\n",
    "    pause : int, optional\n",
    "        Pause between API requests in seconds, defaults to 2\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    gdf : GeoDataFrame\n",
    "        Combined GeoDataFrame of all POIs\n",
    "    \"\"\"\n",
    "    # Get place geometry once to reuse\n",
    "    try:\n",
    "        place_gdf = ox.geocode_to_gdf(place_name)\n",
    "        place_polygon = place_gdf.unary_union\n",
    "        print(f\"Retrieved polygon for {place_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving place geometry: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Set custom timeout for API requests\n",
    "    ox.settings.requests_timeout = timeout\n",
    "    \n",
    "    # Initialize list to store GeoDataFrames\n",
    "    gdfs = []\n",
    "    successful_categories = []\n",
    "    failed_categories = []\n",
    "    \n",
    "    # Iterate through each tag category\n",
    "    for category, values in tqdm(tag_dict.items(), desc=\"Processing categories\"):\n",
    "        # Create a single-key dictionary for this category\n",
    "        single_tag_dict = {category: values}\n",
    "        \n",
    "        try:\n",
    "            # Fetch features using the polygon\n",
    "            gdf = ox.features_from_polygon(place_polygon, tags=single_tag_dict)\n",
    "            gdf = gdf.reset_index()\n",
    "            if not gdf.empty:\n",
    "                # Add category metadata\n",
    "                gdf['poi_category'] = category\n",
    "                \n",
    "                # Convert problematic columns to string to ensure compatibility\n",
    "                for col in gdf.columns:\n",
    "                    if col != 'geometry' and isinstance(gdf[col].iloc[0], (list, dict)):\n",
    "                        gdf[col] = gdf[col].astype(str)\n",
    "                \n",
    "                gdfs.append(gdf)\n",
    "                successful_categories.append(category)\n",
    "                print(f\"Added {len(gdf)} {category} features\")\n",
    "            else:\n",
    "                print(f\"No {category} features found\")\n",
    "                \n",
    "            # Add pause to avoid overwhelming the API\n",
    "            time.sleep(pause)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {category}: {e}\")\n",
    "            failed_categories.append(category)\n",
    "            continue\n",
    "    \n",
    "    # Report on processing results\n",
    "    print(f\"\\nSuccessfully processed {len(successful_categories)} categories\")\n",
    "    if failed_categories:\n",
    "        print(f\"Failed to process {len(failed_categories)} categories: {', '.join(failed_categories)}\")\n",
    "    \n",
    "    # Combine all GeoDataFrames\n",
    "    if gdfs:\n",
    "        try:\n",
    "            combined_gdf = pd.concat(gdfs, ignore_index=True)\n",
    "            # Remove duplicate POIs based on OSM ID\n",
    "            if 'osmid' in combined_gdf.columns:\n",
    "                combined_gdf = combined_gdf.drop_duplicates(subset='osmid')\n",
    "            print(f\"Final dataset contains {len(combined_gdf)} unique features\")\n",
    "            return combined_gdf\n",
    "        except Exception as e:\n",
    "            print(f\"Error combining GeoDataFrames: {e}\")\n",
    "            return gdfs  # Return the list of GeoDataFrames if concatenation fails\n",
    "    else:\n",
    "        print(\"No features collected\")\n",
    "        return gpd.GeoDataFrame(geometry=[], crs=\"EPSG:4326\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gdf_culture_pois = fetch_pois_from_tag_dict(\"Grand Gen√®ve\", culture_pois_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Culture POIs\n",
    "# gdf_culture_pois_2025 = fetch_pois_from_tag_dict(\"Grand Gen√®ve\", culture_pois_dict)\n",
    "\n",
    "# # Education POIs\n",
    "# gdf_education_pois_2025 = fetch_pois_from_tag_dict(\"Grand Gen√®ve\", education_pois_dict)\n",
    "\n",
    "# # Healthcare POIs\n",
    "# gdf_healthcare_pois_2025 = fetch_pois_from_tag_dict(\"Grand Gen√®ve\", healthcare_pois_dict)\n",
    "\n",
    "# # Services POIs\n",
    "# gdf_services_pois_2025 = fetch_pois_from_tag_dict(\"Grand Gen√®ve\", services_pois_dict)\n",
    "\n",
    "# # Transport POIs\n",
    "# gdf_transport_pois_2025 = fetch_pois_from_tag_dict(\"Grand Gen√®ve\", transport_pois_dict)\n",
    "\n",
    "# # Outdoor POIs\n",
    "# gdf_outdoor_pois_2025 = fetch_pois_from_tag_dict(\"Grand Gen√®ve\", outdoor_pois_dict)\n",
    "\n",
    "# # Supplies POIs\n",
    "# gdf_supplies_pois_2025 = fetch_pois_from_tag_dict(\"Grand Gen√®ve\", supplies_pois_dict)\n",
    "\n",
    "# # Restaurant POIs\n",
    "# gdf_restaurant_pois_2025 = fetch_pois_from_tag_dict(\"Grand Gen√®ve\", restaurant_pois_dict)\n",
    "\n",
    "# # Physical POIs\n",
    "# gdf_physical_pois_2025 = fetch_pois_from_tag_dict(\"Grand Gen√®ve\", physical_pois_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import backoff\n",
    "\n",
    "# Add exponential backoff decorator for API resilience\n",
    "@backoff.on_exception(backoff.expo, \n",
    "                     (Exception),\n",
    "                     max_tries=5,\n",
    "                     max_time=300)\n",
    "def fetch_features_with_retry(polygon, tags):\n",
    "    \"\"\"Fetch features with exponential backoff retry logic\"\"\"\n",
    "    return ox.features_from_polygon(polygon, tags=tags)\n",
    "\n",
    "def fetch_pois_from_tag_dict(place_name, tag_dict, timeout=180, pause=5, chunk_size=None):\n",
    "    \"\"\"\n",
    "    Fetch points of interest by iterating through tag categories with improved error handling.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    place_name : str\n",
    "        The name of the place to query (e.g., 'Grand Gen√®ve')\n",
    "    tag_dict : dict\n",
    "        Dictionary of tag categories and their values\n",
    "    timeout : int, optional\n",
    "        Timeout for API requests in seconds, defaults to 180 (increased)\n",
    "    pause : int, optional\n",
    "        Pause between API requests in seconds, defaults to 5 (increased)\n",
    "    chunk_size : int, optional\n",
    "        If provided, splits large areas into chunks of this size in sq km\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    gdf : GeoDataFrame\n",
    "        Combined GeoDataFrame of all POIs\n",
    "    \"\"\"\n",
    "    # Set custom timeout for API requests (increased)\n",
    "    ox.settings.requests_timeout = timeout\n",
    "    ox.settings.overpass_rate_limit = True  # Enable rate limiting\n",
    "    \n",
    "    # Get place geometry once to reuse\n",
    "    try:\n",
    "        place_gdf = ox.geocode_to_gdf(place_name)\n",
    "        place_polygon = place_gdf.unary_union\n",
    "        print(f\"Retrieved polygon for {place_name}\")\n",
    "        \n",
    "        # Optional: Split into chunks if area is large and chunk_size is provided\n",
    "        if chunk_size is not None and place_gdf.to_crs(epsg=3857).area.sum() > chunk_size * 1e6:\n",
    "            from shapely.geometry import box\n",
    "            print(f\"Large area detected. Splitting into chunks...\")\n",
    "            minx, miny, maxx, maxy = place_polygon.bounds\n",
    "            chunks = []\n",
    "            curr_x = minx\n",
    "            while curr_x < maxx:\n",
    "                curr_y = miny\n",
    "                while curr_y < maxy:\n",
    "                    chunk = box(curr_x, curr_y, \n",
    "                                min(curr_x + 0.15, maxx), \n",
    "                                min(curr_y + 0.15, maxy))\n",
    "                    if chunk.intersects(place_polygon):\n",
    "                        chunks.append(chunk.intersection(place_polygon))\n",
    "                    curr_y += 0.15\n",
    "                curr_x += 0.15\n",
    "            print(f\"Area split into {len(chunks)} chunks\")\n",
    "            polygons = chunks\n",
    "        else:\n",
    "            polygons = [place_polygon]\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving place geometry: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Initialize list to store GeoDataFrames\n",
    "    gdfs = []\n",
    "    successful_categories = []\n",
    "    failed_categories = []\n",
    "    \n",
    "    # Iterate through each tag category\n",
    "    for category, values in tqdm(tag_dict.items(), desc=\"Processing categories\"):\n",
    "        # Create a single-key dictionary for this category\n",
    "        single_tag_dict = {category: values}\n",
    "        \n",
    "        category_gdfs = []\n",
    "        try:\n",
    "            # Process each polygon (or chunk)\n",
    "            for i, poly in enumerate(polygons):\n",
    "                if len(polygons) > 1:\n",
    "                    print(f\"Processing chunk {i+1}/{len(polygons)} for {category}\")\n",
    "                \n",
    "                try:\n",
    "                    # Use the retry-enabled function\n",
    "                    gdf = fetch_features_with_retry(poly, single_tag_dict)\n",
    "                    \n",
    "                    if not gdf.empty:\n",
    "                        gdf = gdf.reset_index()\n",
    "                        # Add category metadata\n",
    "                        gdf['poi_category'] = category\n",
    "                        \n",
    "                        # Convert problematic columns to string\n",
    "                        for col in gdf.columns:\n",
    "                            if col != 'geometry' and isinstance(gdf[col].iloc[0], (list, dict)):\n",
    "                                gdf[col] = gdf[col].astype(str)\n",
    "                        \n",
    "                        category_gdfs.append(gdf)\n",
    "                        print(f\"  Added {len(gdf)} {category} features from chunk {i+1}\")\n",
    "                    \n",
    "                    # Add pause between chunk requests\n",
    "                    time.sleep(pause)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"  Error in chunk {i+1} for {category}: {e}\")\n",
    "                    # Continue with next chunk instead of failing entire category\n",
    "            \n",
    "            # Combine all chunks for this category\n",
    "            if category_gdfs:\n",
    "                category_combined = pd.concat(category_gdfs, ignore_index=True)\n",
    "                gdfs.append(category_combined)\n",
    "                successful_categories.append(category)\n",
    "                print(f\"Added total of {len(category_combined)} {category} features\")\n",
    "            else:\n",
    "                print(f\"No {category} features found in any chunk\")\n",
    "                \n",
    "            # Add pause between categories\n",
    "            time.sleep(pause)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {category}: {e}\")\n",
    "            failed_categories.append(category)\n",
    "            continue\n",
    "    \n",
    "    # Report on processing results\n",
    "    print(f\"\\nSuccessfully processed {len(successful_categories)} categories\")\n",
    "    if failed_categories:\n",
    "        print(f\"Failed to process {len(failed_categories)} categories: {', '.join(failed_categories)}\")\n",
    "    \n",
    "    # Combine all GeoDataFrames\n",
    "    if gdfs:\n",
    "        try:\n",
    "            combined_gdf = pd.concat(gdfs, ignore_index=True)\n",
    "            # Remove duplicate POIs based on OSM ID\n",
    "            if 'osmid' in combined_gdf.columns:\n",
    "                combined_gdf = combined_gdf.drop_duplicates(subset='osmid')\n",
    "            print(f\"Final dataset contains {len(combined_gdf)} unique features\")\n",
    "            return combined_gdf\n",
    "        except Exception as e:\n",
    "            print(f\"Error combining GeoDataFrames: {e}\")\n",
    "            return gdfs  # Return the list of GeoDataFrames if concatenation fails\n",
    "    else:\n",
    "        print(\"No features collected\")\n",
    "        return gpd.GeoDataFrame(geometry=[], crs=\"EPSG:4326\")\n",
    "\n",
    "# Install backoff library if not already installed\n",
    "# !pip install backoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage\n",
    "gdf_culture_pois_2025 = fetch_pois_from_tag_dict(\n",
    "    \"Grand Gen√®ve\", \n",
    "    culture_pois_dict, \n",
    "    timeout=180,     # Increased timeout\n",
    "    pause=10,        # Increased pause between requests\n",
    "    chunk_size=50    # Optional: enable chunking for large areas\n",
    ")\n",
    "# Education POIs\n",
    "gdf_education_pois_2025 = fetch_pois_from_tag_dict(\"Grand Gen√®ve\", education_pois_dict, \n",
    "    timeout=90,     # Increased timeout\n",
    "    pause=10,        # Increased pause between requests\n",
    "    chunk_size=25 )   # Optional: enable chunking for large areas)\n",
    "\n",
    "# Healthcare POIs\n",
    "gdf_healthcare_pois_2025 = fetch_pois_from_tag_dict(\"Grand Gen√®ve\", healthcare_pois_dict, \n",
    "    timeout=90,     # Increased timeout\n",
    "    pause=5,        # Increased pause between requests\n",
    "    chunk_size=25 )   # Optional: enable chunking for large areas)\n",
    "\n",
    "# Services POIs\n",
    "gdf_services_pois_2025 = fetch_pois_from_tag_dict(\"Grand Gen√®ve\", services_pois_dict, \n",
    "    timeout=90,     # Increased timeout\n",
    "    pause=5,        # Increased pause between requests\n",
    "    chunk_size=25)    # Optional: enable chunking for large areas)\n",
    "\n",
    "# Transport POIs\n",
    "gdf_transport_pois_2025 = fetch_pois_from_tag_dict(\"Grand Gen√®ve\", transport_pois_dict, \n",
    "    timeout=90,     # Increased timeout\n",
    "    pause=5,        # Increased pause between requests\n",
    "    chunk_size=25)    # Optional: enable chunking for large areas)\n",
    "\n",
    "# Outdoor POIs\n",
    "gdf_outdoor_pois_2025 = fetch_pois_from_tag_dict(\"Grand Gen√®ve\", outdoor_pois_dict, \n",
    "    timeout=90,     # Increased timeout\n",
    "    pause=5,        # Increased pause between requests\n",
    "    chunk_size=25)    # Optional: enable chunking for large areas)\n",
    "\n",
    "# Supplies POIs\n",
    "gdf_supplies_pois_2025 = fetch_pois_from_tag_dict(\"Grand Gen√®ve\", supplies_pois_dict, \n",
    "    timeout=90,     # Increased timeout\n",
    "    pause=10,        # Increased pause between requests\n",
    "    chunk_size=25)    # Optional: enable chunking for large areas)\n",
    "\n",
    "# Restaurant POIs\n",
    "gdf_restaurant_pois_2025 = fetch_pois_from_tag_dict(\"Grand Gen√®ve\", restaurant_pois_dict, \n",
    "    timeout=90,     # Increased timeout\n",
    "    pause=5,        # Increased pause between requests\n",
    "    chunk_size=25)    # Optional: enable chunking for large areas)\n",
    "\n",
    "# Physical POIs\n",
    "gdf_physical_pois_2025 = fetch_pois_from_tag_dict(\"Grand Gen√®ve\", physical_pois_dict, \n",
    "    timeout=90,     # Increased timeout\n",
    "    pause=5,        # Increased pause between requests\n",
    "    chunk_size=25)    # Optional: enable chunking for large areas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_culture_pois_2025.drop(['nodes', 'ways'], axis=1).to_parquet('../data/15min_city/raw_osm_culture_pois.parquet')\n",
    "gdf_education_pois_2025.drop(['nodes', 'ways'], axis=1).to_parquet('../data/15min_city/raw_osm_education_pois.parquet')\n",
    "gdf_healthcare_pois_2025.drop(['nodes', 'ways'], axis=1).to_parquet('../data/15min_city/raw_osm_healthcare_pois.parquet')\n",
    "gdf_physical_pois_2025.drop(['nodes', 'ways'], axis=1).to_parquet('../data/15min_city/raw_osm_physical_pois.parquet')\n",
    "gdf_services_pois_2025.drop(['nodes', 'ways'], axis=1).to_parquet('../data/15min_city/raw_osm_services_pois.parquet')\n",
    "gdf_transport_pois_2025.drop(['nodes', 'ways'], axis=1).to_parquet('../data/15min_city/raw_osm_transport_pois.parquet')\n",
    "gdf_outdoor_pois_2025.drop(['nodes', 'ways'], axis=1).to_parquet('../data/15min_city/raw_osm_outdoor_pois.parquet')\n",
    "gdf_supplies_pois_2025.drop(['nodes', 'ways'], axis=1).to_parquet('../data/15min_city/raw_osm_supplies_pois.parquet')\n",
    "gdf_restaurant_pois_2025.drop(['nodes'], axis=1).to_parquet('../data/15min_city/raw_osm_restaurant_pois.parquet')\n",
    "gdf_physical_pois_2025.drop(['nodes', 'ways'], axis=1).to_parquet('../data/15min_city/raw_osm_physical_pois.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_culture_pois_2025 = pd.read_parquet('../data/15min_city/raw_osm_culture_pois.parquet')\n",
    "gdf_education_pois_2025 = pd.read_parquet('../data/15min_city/raw_osm_education_pois.parquet')\n",
    "gdf_healthcare_pois_2025 = pd.read_parquet('../data/15min_city/raw_osm_healthcare_pois.parquet')\n",
    "gdf_physical_pois_2025 = pd.read_parquet('../data/15min_city/raw_osm_physical_pois.parquet')\n",
    "gdf_services_pois_2025 = pd.read_parquet('../data/15min_city/raw_osm_services_pois.parquet')\n",
    "gdf_transport_pois_2025 = pd.read_parquet('../data/15min_city/raw_osm_transport_pois.parquet')\n",
    "gdf_outdoor_pois_2025 = pd.read_parquet('../data/15min_city/raw_osm_outdoor_pois.parquet')\n",
    "gdf_supplies_pois_2025 = pd.read_parquet('../data/15min_city/raw_osm_supplies_pois.parquet')\n",
    "gdf_restaurant_pois_2025 = pd.read_parquet('../data/15min_city/raw_osm_restaurant_pois.parquet')\n",
    "gdf_physical_pois_2025 = pd.read_parquet('../data/15min_city/raw_osm_physical_pois.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## OSM data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_geometries(df):\n",
    "    from shapely import wkb\n",
    "    if isinstance(df['geometry'].iloc[0], bytes):\n",
    "        df['geometry'] = df['geometry'].apply(lambda x: wkb.loads(x) if isinstance(x, bytes) else x)\n",
    "    return gpd.GeoDataFrame(df, geometry='geometry', crs=4326)\n",
    "    \n",
    "def add_categories(gdf, category_name, category_dict):\n",
    "    \"\"\"\n",
    "    Add 'category' and 'subcategory' columns to a GeoDataFrame based on a dictionary.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    gdf : GeoDataFrame\n",
    "        The GeoDataFrame to categorize\n",
    "    category_name : str\n",
    "        The main category name (e.g., 'physical')\n",
    "    category_dict : dict\n",
    "        Dictionary where keys are column names and values are lists of values to match\n",
    "        or True to match any non-null value\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    GeoDataFrame\n",
    "        The input GeoDataFrame with 'category' and 'subcategory' columns added\n",
    "    \"\"\"\n",
    "    # Create copies of the GeoDataFrame and add new columns\n",
    "    result_gdf = gdf.copy()\n",
    "    result_gdf['category'] = None\n",
    "    result_gdf['subcategory'] = None\n",
    "    \n",
    "    # Track rows that have been categorized\n",
    "    categorized_mask = pd.Series(False, index=result_gdf.index)\n",
    "    \n",
    "    # Iterate through dictionary items\n",
    "    for column_name, values in category_dict.items():\n",
    "        # Check if column exists in the GeoDataFrame\n",
    "        if column_name not in result_gdf.columns:\n",
    "            continue\n",
    "            \n",
    "        if values is True:\n",
    "            # If values is True, match any non-null value in the column\n",
    "            mask = ~result_gdf[column_name].isna() & ~categorized_mask\n",
    "        else:\n",
    "            # Match specific values in the column\n",
    "            mask = result_gdf[column_name].isin(values) & ~categorized_mask\n",
    "        \n",
    "        # Update category and subcategory for matching rows\n",
    "        result_gdf.loc[mask, 'category'] = category_name\n",
    "        result_gdf.loc[mask, 'subcategory'] = column_name\n",
    "        \n",
    "        # Update the mask of categorized rows\n",
    "        categorized_mask = categorized_mask | mask\n",
    "    result_gdf = result_gdf.to_crs(2056)\n",
    "    result_gdf['geometry'] = result_gdf['geometry'].centroid\n",
    "    result_gdf['lon'] = result_gdf.to_crs(4326).geometry.x\n",
    "    result_gdf['lat'] = result_gdf.to_crs(4326).geometry.y\n",
    "    return result_gdf.reset_index(drop=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_culture_pois_2025 = pd.read_parquet('../data/15min_city/raw_osm_culture_pois.parquet')\n",
    "gdf_education_pois_2025 = pd.read_parquet('../data/15min_city/raw_osm_education_pois.parquet')\n",
    "gdf_healthcare_pois_2025 = pd.read_parquet('../data/15min_city/raw_osm_healthcare_pois.parquet')\n",
    "gdf_physical_pois_2025 = pd.read_parquet('../data/15min_city/raw_osm_physical_pois.parquet')\n",
    "gdf_services_pois_2025 = pd.read_parquet('../data/15min_city/raw_osm_services_pois.parquet')\n",
    "gdf_transport_pois_2025 = pd.read_parquet('../data/15min_city/raw_osm_transport_pois.parquet')\n",
    "gdf_outdoor_pois_2025 = pd.read_parquet('../data/15min_city/raw_osm_outdoor_pois.parquet')\n",
    "gdf_supplies_pois_2025 = pd.read_parquet('../data/15min_city/raw_osm_supplies_pois.parquet')\n",
    "gdf_restaurant_pois_2025 = pd.read_parquet('../data/15min_city/raw_osm_restaurant_pois.parquet')\n",
    "gdf_physical_pois_2025 = pd.read_parquet('../data/15min_city/raw_osm_physical_pois.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix geometries if needed\n",
    "gdf_culture_pois_2025 = fix_geometries(gdf_culture_pois_2025)\n",
    "gdf_education_pois_2025 = fix_geometries(gdf_education_pois_2025)\n",
    "gdf_healthcare_pois_2025 = fix_geometries(gdf_healthcare_pois_2025)\n",
    "gdf_physical_pois_2025 = fix_geometries(gdf_physical_pois_2025)\n",
    "gdf_services_pois_2025 = fix_geometries(gdf_services_pois_2025)\n",
    "gdf_transport_pois_2025 = fix_geometries(gdf_transport_pois_2025)\n",
    "gdf_outdoor_pois_2025 = fix_geometries(gdf_outdoor_pois_2025)\n",
    "gdf_supplies_pois_2025 = fix_geometries(gdf_supplies_pois_2025)\n",
    "gdf_restaurant_pois_2025 = fix_geometries(gdf_restaurant_pois_2025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_physical_pois_2025_clean = add_categories(gdf_physical_pois_2025, 'physical', physical_pois_dict)\n",
    "gdf_culture_pois_2025_clean = add_categories(gdf_culture_pois_2025, 'culture', culture_pois_dict)\n",
    "gdf_education_pois_2025_clean = add_categories(gdf_education_pois_2025, 'education', education_pois_dict)\n",
    "gdf_healthcare_pois_2025_clean = add_categories(gdf_healthcare_pois_2025, 'healthcare', healthcare_pois_dict)\n",
    "gdf_services_pois_2025_clean = add_categories(gdf_services_pois_2025, 'services', services_pois_dict)\n",
    "gdf_transport_pois_2025_clean = add_categories(gdf_transport_pois_2025, 'transport', transport_pois_dict)\n",
    "gdf_outdoor_pois_2025_clean = add_categories(gdf_outdoor_pois_2025, 'outdoor', outdoor_pois_dict)\n",
    "gdf_supplies_pois_2025_clean = add_categories(gdf_supplies_pois_2025, 'supplies', supplies_pois_dict)\n",
    "gdf_restaurant_pois_2025_clean = add_categories(gdf_restaurant_pois_2025, 'restaurant', restaurant_pois_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra step : exclude private swimming pools\n",
    "gdf_physical_pois_2025_clean = gdf_physical_pois_2025_clean[gdf_physical_pois_2025_clean.leisure != 'swimming_pool'].reset_index(drop=True)\n",
    "# Extra step : exclude private gardens\n",
    "gdf_outdoor_pois_2025_clean = gdf_outdoor_pois_2025_clean[gdf_outdoor_pois_2025_clean.leisure != 'garden'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all the cleaned dataframes (optional)\n",
    "gdfs_clean = [\n",
    "    gdf_physical_pois_2025_clean[['osmid','category','subcategory', 'lon', 'lat', 'geometry']],\n",
    "    gdf_culture_pois_2025_clean[['osmid','category','subcategory', 'lon', 'lat', 'geometry']],\n",
    "    gdf_education_pois_2025_clean[['osmid','category','subcategory', 'lon', 'lat', 'geometry']],\n",
    "    gdf_healthcare_pois_2025_clean[['osmid','category','subcategory', 'lon', 'lat', 'geometry']],\n",
    "    gdf_services_pois_2025_clean[['osmid','category','subcategory', 'lon', 'lat', 'geometry']],\n",
    "    gdf_transport_pois_2025_clean[['osmid','category','subcategory', 'lon', 'lat', 'geometry']],\n",
    "    gdf_outdoor_pois_2025_clean[['osmid','category','subcategory', 'lon', 'lat', 'geometry']],\n",
    "    gdf_supplies_pois_2025_clean[['osmid','category','subcategory', 'lon', 'lat', 'geometry']],\n",
    "    gdf_restaurant_pois_2025_clean[['osmid','category','subcategory', 'lon', 'lat', 'geometry']]\n",
    "]\n",
    "\n",
    "# Concatenate all cleaned dataframes into one (optional)\n",
    "gdf_all_pois_clean = pd.concat(gdfs_clean, ignore_index=True)\n",
    "\n",
    "# Save the results (optional)\n",
    "for gdf_clean, category in zip(gdfs_clean, ['physical', 'culture', 'education', 'healthcare', \n",
    "                                           'services', 'transport', 'outdoor', 'supplies', 'restaurant']):\n",
    "    gdf_clean.to_parquet(f'../data/15min_city/processed_osm_{category}_pois.parquet')\n",
    "\n",
    "# Save the combined dataframe (optional)\n",
    "gdf_all_pois_clean.to_parquet('../data/15min_city/processed_osm_all_pois.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
